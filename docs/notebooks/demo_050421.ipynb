{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4cb8c71",
   "metadata": {},
   "source": [
    "# Transcriptomic Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0917974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import transcriptomic_clustering as tc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d25bde3",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "transcriptomic_clustering functions operate on Annotation Data, from part of scanpy/anndata. This supports both dense sparse matrices, and additional .['obs'], .['obsm'] and .['var'], [.varm] fields for storing information about the observations (cells) and variables (genes). Data is stored as an HDF5 file, allowing for both in-memory and file-backed operations. For more information, see https://anndata.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-province",
   "metadata": {},
   "source": [
    "<img src=\"https://falexwolf.de/img/scanpy/anndata.svg\" width=\"480\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eeb36ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 1809 × 24057 backed at 'data/tasic2016counts_csr.h5ad'\n",
      "    obs: 'cells'\n",
      "<HDF5 sparse dataset: format 'csr', shape (1809, 24057), type '<f4'>\n"
     ]
    }
   ],
   "source": [
    "# Data is load into memory. Add 'r' for reading and 'r+' for modifying filebacked data\n",
    "tasic_adata = sc.read_h5ad('./data/tasic2016counts_sparse.h5ad', backed='r')\n",
    "print(tasic_adata)\n",
    "print(tasic_adata.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a803c0",
   "metadata": {},
   "source": [
    "## Running the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911efc52",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "To start, we normalize the expression matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ca5f138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 1809 × 24057 backed at 'data/normalized2.h5ad'\n",
       "    obs: 'cells'\n",
       "    uns: 'normalized'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_adata = tc.normalize(tasic_adata, copy_to='./data/normalized2.h5ad')\n",
    "normalized_adata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98dc08d",
   "metadata": {},
   "source": [
    "### Select Highly Variable Genes\n",
    "\n",
    "We extract highly variable genes (HVGs) to further reduce the dimensionality of the dataset and include only the most informative genes. HVGs will be used for the following dimensionality reduction and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48632772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 1809 × 24057 backed at 'data/normalized2.h5ad'\n",
       "    obs: 'cells'\n",
       "    var: 'highly_variable'\n",
       "    uns: 'normalized', 'hvg'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute means and variances:\n",
    "means, variances, gene_mask = tc.means_vars_genes(adata=normalized_adata)\n",
    "\n",
    "# Find highly variable genes:\n",
    "tc.highly_variable_genes(adata=normalized_adata, \n",
    "                         means=means, variances=variances, \n",
    "                         gene_mask=gene_mask, max_genes=3000)\n",
    "normalized_adata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ab8cc0",
   "metadata": {},
   "source": [
    "### PCA analysis\n",
    "Now we can do principal component analysis on a subset of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee0b30d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do PCA on 1000 random cells and highly variable genes\n",
    "(components, explained_variance_ratio, explained_variance) = \\\n",
    "    tc.pca(normalized_adata, cell_select=1000, use_highly_variable=True, svd_solver='arpack')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf66e60",
   "metadata": {},
   "source": [
    "## General API patterns\n",
    "### Appending to AnnData objects\n",
    "Many functions in the transcriptomic package calculate features of the data. By default, they will return the values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "137c47e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 1809 × 24057\n",
       "    obs: 'cells'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata = sc.read_h5ad('./data/tasic2016counts_csr.h5ad')\n",
    "hvg_df = tc.highly_variable_genes(adata, \n",
    "                         means=means, \n",
    "                         variances=variances, \n",
    "                         gene_mask=gene_mask, \n",
    "                         max_genes=3000, \n",
    "                         annotate=False)\n",
    "\n",
    "adata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcb416f",
   "metadata": {},
   "source": [
    "However, some functions allow you to add the data to the AnnData object instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d35da91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 1809 × 24057\n",
       "    obs: 'cells'\n",
       "    var: 'highly_variable'\n",
       "    uns: 'hvg'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc.highly_variable_genes(adata, \n",
    "                         means=means, \n",
    "                         variances=variances, \n",
    "                         gene_mask=gene_mask, \n",
    "                         max_genes=3000, \n",
    "                         annotate=True)\n",
    "\n",
    "adata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fbd6c9",
   "metadata": {},
   "source": [
    "### Modifying AnnData in Place or Making a Copy\n",
    "A function that modifies AnnData will by default make a copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d382c311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# By default will create a new object:\n",
    "tasic_adata_inmemory = sc.read_h5ad('./data/tasic2016counts_csr.h5ad')\n",
    "normalized_adata = tc.normalize(tasic_adata_inmemory)\n",
    "print(normalized_adata is tasic_adata_inmemory) # False: different objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827bdf30",
   "metadata": {},
   "source": [
    "But passing `inplace=True` will overwrite instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d98b8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sergeyg/repos/transcriptomic_clustering/transcriptomic_clustering/normalization.py:96: UserWarning: Modifying data in place.\n",
      "  warnings.warn(\"Modifying data in place.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# But can also modify inplace:\n",
    "normalized_adata = tc.normalize(tasic_adata_inmemory, inplace=True)\n",
    "print(normalized_adata is tasic_adata_inmemory) # True: same  objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6220d850",
   "metadata": {},
   "source": [
    "### Managing Memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b028ba",
   "metadata": {},
   "source": [
    "#### Available Memory and Setting Memory Limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1305774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4089927673339844"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc.memory.get_available_system_memory_GB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c265f966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:           15Gi        10Gi       1.9Gi       826Mi       2.6Gi       3.4Gi\r\n",
      "Swap:         2.0Gi       1.7Gi       356Mi\r\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69a0c61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.memory.set_memory_limit(percent_current_available=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "demonstrated-guinea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.memory.set_memory_limit(GB=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d06ae183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3396278381347656"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc.memory.get_available_memory_GB()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72925a91",
   "metadata": {},
   "source": [
    "#### Chunked Processing\n",
    "By default, chunked processing is turned-off, and tc won't do any memory management for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0238d581",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "The process: `normalize` cannot fit in memory, but could be done using chunking.\nSet transcriptomic_clustering.memory.allow_chunking=True",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-6efc6f072a0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnormalized_adata_backed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasic_adata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./data/normalized3.h5ad'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/repos/transcriptomic_clustering/transcriptomic_clustering/normalization.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(adata, inplace, chunk_size, copy_to)\u001b[0m\n\u001b[1;32m     84\u001b[0m             )\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0madata_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_backed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/transcriptomic_clustering/transcriptomic_clustering/normalization.py\u001b[0m in \u001b[0;36mnormalize_backed\u001b[0;34m(adata, chunk_size, copy_to)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0moutput_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_memory_est\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mpercent_allowed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mprocess_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'normalize'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m     )\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/transcriptomic_clustering/transcriptomic_clustering/utils/memory.py\u001b[0m in \u001b[0;36mestimate_chunk_size\u001b[0;34m(self, adata, process_memory, output_memory, percent_allowed, process_name)\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0moutput_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mpercent_allowed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpercent_allowed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0mprocess_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         )\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_chunk_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_chunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/transcriptomic_clustering/transcriptomic_clustering/utils/memory.py\u001b[0m in \u001b[0;36mestimate_n_chunks\u001b[0;34m(self, process_memory, output_memory, percent_allowed, process_name)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_chunking\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             raise MemoryError(\n\u001b[0;32m--> 127\u001b[0;31m                 \u001b[0;34mf'The process: `{process_name}` cannot fit in memory, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m                 \u001b[0;34m'but could be done using chunking.\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;34m'Set transcriptomic_clustering.memory.allow_chunking=True'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: The process: `normalize` cannot fit in memory, but could be done using chunking.\nSet transcriptomic_clustering.memory.allow_chunking=True"
     ]
    }
   ],
   "source": [
    "normalized_adata_backed = tc.normalize(tasic_adata, copy_to='./data/normalized3.h5ad')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a761725f",
   "metadata": {},
   "source": [
    "But setting `tc.memory.allow_chunking = True`, you can enable automatic chunked processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "noticed-laser",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.memory.allow_chunking=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "homeless-venue",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing: 100%|██████████| 5/5 [00:01<00:00,  3.98it/s]\n"
     ]
    }
   ],
   "source": [
    "normalized_adata_backed = tc.normalize(tasic_adata, copy_to='./data/normalized4.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "sonic-journalist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 1809 × 24057 backed at 'data/normalized4.h5ad'\n",
       "    obs: 'cells'\n",
       "    uns: 'normalized'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_adata_backed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dried-independence",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing: 100%|██████████| 10/10 [00:01<00:00,  7.65it/s]\n"
     ]
    }
   ],
   "source": [
    "normalized2_adata_backed = tc.normalize(tasic_adata, copy_to='./data/normalized14.h5ad',chunk_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "acquired-humor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 1809 × 24057 backed at 'data/normalized14.h5ad'\n",
       "    obs: 'cells'\n",
       "    uns: 'normalized'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized2_adata_backed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-travel",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}