{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57f8370b",
   "metadata": {},
   "source": [
    "## Running Iterative Clustering\n",
    "This jupyter notebook can be used to run iterative clustering on large data sets\n",
    "\n",
    "### Before you get started\n",
    "#### Harddrive space\n",
    "This runs best on a machine with a large local ssd. Here is an example of the storage space you'll need\n",
    "* 250GB - raw_counts.h5ad\n",
    "* 250GB - normalized.h5ad (if you already have this file, you don't need the raw_counts.h5ad)\n",
    "* 250GB - space used by iterative clustering to store temporary files\n",
    "\n",
    "#### Running Jupyter Notebook through ssh\n",
    "You can run jupyter notebook through SSH! \n",
    "1) ssh into the remote machine you want to run it on\n",
    "2) activate your transcriptomic clustering environment\n",
    "3) run `nohup jupyter notebook -no-browser --port=1234 &`. This will start up the jupyter notebook server and prevent it from ending if your SSH connection gets terminated. Alternatively, you could use programs like `tmux` or `screen`. Copy the address it shows for step 5\n",
    "4) back on your machine, run `ssh -NL 1234:localhost:1234 username@remote-machine` to open an SSH tunnel\n",
    "5) enter the address from step 4 (should be something like 'http://localhost:1234/?token=8d186032bbbe095b294789e863b065a546fcc15b68683c99' Now you should be able to interact with the notebook on the remote machine!\n",
    "\n",
    "#### Temporary Directories\n",
    "Because AnnData doesn't support multiple views of filebacked data (e.g. subset=adata[1:5], subset[1:2]), we have to create temporary files for each cluster until we can store the whole cluster into memory. We store these files in the temporary directory - it will mostly cleanup after itself, but always check and remove old tmp files to keep your harddrive free\n",
    "\n",
    "#### Run normalized data first\n",
    "and save it so you don't need to rerun if you restart iterative clustering. It takes 60-90 minutes to normalize the data, but you don't need to repeat it if it's already been created. Just start directly at iterative clustering.\n",
    "\n",
    "#### If SSH disconnects\n",
    "Just reopen the SSH Tunnel (step 4 and 5), and you'll be able to see the notebook. However, due to a known unresolved issue, when you reopen the notebook you will no longer get log messages from iter_clust. As a work around, you can monitor progress with commands like `top`, `ls` the temporary directory for new files being created, or you can try [this workaround](https://github.com/jupyter/jupyter/issues/83#issuecomment-622984009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee4098b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Application.log_level='INFO'\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902e2db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import scipy as scp\n",
    "import scanpy as sc\n",
    "import transcriptomic_clustering as tc\n",
    "from transcriptomic_clustering.iterative_clustering import (build_cluster_dict, iter_clust, OnestepKwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7c782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup input/output files\n",
    "output_file = os.path.expanduser('clusters.json')\n",
    "\n",
    "path_to_adata = './data/tasic2016counts_sparse.h5ad'\n",
    "adata = sc.read_h5ad(path_to_adata, backed='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98669ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set memory params\n",
    "tc.memory.set_memory_limit(GB=1)\n",
    "tc.memory.allow_chunking = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d91c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign kwargs. Any unassigned args will be set to their respective function defaults\n",
    "merge_clusters_kwargs = {\n",
    "    'thresholds': {\n",
    "        'q1_thresh': 0.5,\n",
    "        'q2_thresh': None,\n",
    "        'cluster_size_thresh': 15,\n",
    "        'qdiff_thresh': 0.7,\n",
    "        'padj_thresh': 0.05,\n",
    "        'lfc_thresh': 1.0,\n",
    "        'score_thresh': 200,\n",
    "        'low_thresh': 1\n",
    "    },\n",
    "    'de_method': 'ebayes'\n",
    "}\n",
    "onestep_kwargs = OnestepKwargs(merge_clusters_kwargs=merge_clusters_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83854b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove old tmp_dir and make new one\n",
    "try:\n",
    "    shutil.rmtree(tmp_dir)\n",
    "except NameError as e:\n",
    "    pass # tmp_dir didn't exist\n",
    "tmp_dir = tempfile.mkdtemp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d997e886",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# normalize adata\n",
    "norm_adata_path = os.path.join(tmp_dir, 'normalized.h5ad')\n",
    "normalized_adata = tc.normalize(adata,copy_to=norm_adata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a8fdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run clustering\n",
    "clusters = iter_clust(\n",
    "    normalized_adata,\n",
    "    min_samples=4,\n",
    "    onestep_kwargs=onestep_kwargs,\n",
    "    random_seed=123,\n",
    "    tmp_dir=tmp_dir\n",
    ")\n",
    "cluster_dict = build_cluster_dict(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95b7fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c964d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_by_obs = np.zeros(normalized_adata.n_obs, dtype=int)\n",
    "for cluster, obs in cluster_dict.items():\n",
    "    cluster_by_obs[obs] = cluster\n",
    "cluster_means, _, _ = tc.get_cluster_means(normalized_adata, cluster_dict, cluster_by_obs)\n",
    "linkage, labels = tc.hclust(cluster_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893b243d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "dn = scp.cluster.hierarchy.dendrogram(linkage, labels=labels)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aeafcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file, 'w') as f:\n",
    "    json.dump(cluster_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3236564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(tmp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc190716",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tc] *",
   "language": "python",
   "name": "conda-env-tc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
